{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-be014968-2deb-4a59-9200-74f40ce602aa;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 699ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-be014968-2deb-4a59-9200-74f40ce602aa\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/14ms)\n",
      "23/05/01 18:12:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-kafka-streaming\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"). \\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "def update_dict(batch_df, epoch_id):\n",
    "    print(batch_df)\n",
    "    data_dict['foo'] = {}\n",
    "    # Convert the batch DataFrame to a list of dictionaries\n",
    "    batch_list = batch_df.rdd.map(lambda x: x.asDict()).collect()\n",
    "    for record in batch_list:\n",
    "        print(record)\n",
    "        block_id = record[\"id\"]\n",
    "        log_type = record[\"type\"]\n",
    "        if block_id not in data_dict:\n",
    "            data_dict[block_id] = {}\n",
    "        if log_type not in data_dict[block_id]:\n",
    "            data_dict[block_id][log_type] = 1\n",
    "        else:\n",
    "            data_dict[block_id][log_type] += 1\n",
    "            \n",
    "# block_1; {log1: 3, log:4}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_type_index(event_type):\n",
    "    if event_type == 'PacketResponder * from block * terminating':\n",
    "        return 0\n",
    "    if event_type == 'Received block * from *':\n",
    "        return 1\n",
    "    if event_type == 'Verification succeeded for *':\n",
    "        return 2\n",
    "    if event_type == 'Receiving block * src: * des: *':\n",
    "        return 3\n",
    "    if event_type == 'Served block * to *':\n",
    "        return 4\n",
    "    if event_type == 'blockMap updated: * is added to *':\n",
    "        return 5\n",
    "    if event_type == 'block * allocated':\n",
    "        return 6\n",
    "    if event_type == '* is added to invalidSet of *':\n",
    "        return 7\n",
    "    if event_type == 'ask * to replicate * to datanode(s) *':\n",
    "        return 8\n",
    "    if event_type == 'Transmitted block * to *':\n",
    "        return 9\n",
    "    if event_type == 'Starting thread to transfer block * to *':\n",
    "        return 10\n",
    "    if event_type == 'Deleting block * file *':\n",
    "        return 11\n",
    "    if event_type == 'Unexpected error trying to delete block *':\n",
    "        return 12\n",
    "    if event_type == 'Got exception while serving * to *':\n",
    "        return 13\n",
    "    \n",
    "    return 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_dict = {}\n",
    "\n",
    "def create_event_vector(data_dict):\n",
    "    for block_id, event_dict in data_dict.items():\n",
    "        event_count_vector = [0] * 15  # Replace 15 with the total number of event types\n",
    "\n",
    "        for event_type, frequency in event_dict.items():\n",
    "            index = get_event_type_index(event_type)\n",
    "            event_count_vector[index] = frequency\n",
    "\n",
    "        event_count_dict[block_id] = event_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_streamed_raw = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "  .option(\"subscribe\", \"topic_test\")\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# convert byte stream to string\n",
    "df_streamed_kv = (df_streamed_raw\n",
    "    .withColumn(\"key\", df_streamed_raw[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df_streamed_raw[\"value\"].cast(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"type\", StringType())\n",
    "])\n",
    "\n",
    "# Parse the events from JSON format\n",
    "df_parsed = (df_streamed_kv\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", event_schema))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted = (df_parsed.select(\n",
    "    col(\"value.id\").alias(\"id\")\n",
    "    ,col(\"value.type\").alias(\"type\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/01 18:12:41 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4981b264-5082-42fd-aad7-b3f4e84eb867. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "test_query = (df_formatted \n",
    "              .writeStream \\\n",
    "              .format(\"memory\") # output to memory \\\n",
    "              .outputMode(\"append\") \n",
    "              .queryName(\"test_query_table\")  # Name of the in memory table \\\n",
    "              .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "anomaly_map = {}\n",
    "\n",
    "def fill_anomaly_map(file):\n",
    "    with open(file, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            block_id = row[\"BlockId\"]\n",
    "            block_id = block_id.replace(\"blk_\", \"\")\n",
    "            label = row[\"Label\"]\n",
    "            if label == \"Anomaly\":\n",
    "                anomaly_map[block_id] = 1\n",
    "            elif label == \"Normal\":\n",
    "                anomaly_map[block_id] = 0\n",
    "\n",
    "\n",
    "fill_anomaly_map('input_1.csv')\n",
    "fill_anomaly_map('input_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside kmeans clustering with input length:  1862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score till now: 0.6885751968804955\n",
      "Precision score till now: 0.8038365583546306\n",
      "Recall score till now: 0.6401024130190797\n",
      "Confusion Matrix:\n",
      "True Negatives: 1769\n",
      "False Positives: 13\n",
      "False Negatives: 57\n",
      "True Positives: 23\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside kmeans clustering with input length:  1911\n",
      "F1 score till now: 0.6925850224266167\n",
      "Precision score till now: 0.8550102951269732\n",
      "Recall score till now: 0.63636603306003\n",
      "Confusion Matrix:\n",
      "True Negatives: 1820\n",
      "False Positives: 8\n",
      "False Negatives: 60\n",
      "True Positives: 23\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside kmeans clustering with input length:  1929\n",
      "F1 score till now: 0.6965477461673405\n",
      "Precision score till now: 0.8807622504537205\n",
      "Recall score till now: 0.6369290814395175\n",
      "Confusion Matrix:\n",
      "True Negatives: 1840\n",
      "False Positives: 6\n",
      "False Negatives: 60\n",
      "True Positives: 23\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside kmeans clustering with input length:  1956\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "def dicter(result):\n",
    "    result_dict = defaultdict(dict)\n",
    "\n",
    "    # Convert the result DataFrame to a list of Row objects\n",
    "    rows = result.collect()\n",
    "\n",
    "    for row in rows:\n",
    "        block_id = row[\"id\"]\n",
    "        event_type = row[\"type\"]\n",
    "        count = row[\"count(1)\"]\n",
    "        result_dict[block_id][event_type] = count\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "i = 0\n",
    "accuracy = []\n",
    "while True:\n",
    "    print(i)\n",
    "    i = i + 1\n",
    "    result = spark.sql(\"select id, type, count(*) from test_query_table group by id, type\")\n",
    "    result_dict=dicter(result)\n",
    "    create_event_vector(result_dict)\n",
    "    inner_list1 = []\n",
    "    inner_list2 = []\n",
    "    inner_list3 = []\n",
    "    for key, value in event_count_dict.items():\n",
    "        if key in anomaly_map:\n",
    "            inner_list1.append(key)\n",
    "            inner_list2.append(value)\n",
    "            inner_list3.append(int(anomaly_map[key]))\n",
    "        \n",
    "    per_anomalies = sum(inner_list3)/len(inner_list3)\n",
    "    algo1(inner_list1,inner_list2,inner_list3,per_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo1(blk_num,input_data,true_labels, per_anomalies):\n",
    "    print('Inside kmeans clustering with input length: ', len(blk_num))\n",
    "\n",
    "    # Convert the input data to a NumPy array\n",
    "    data = np.array(input_data)\n",
    "\n",
    "    # Scale the data to normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # Define the number of clusters you want to create\n",
    "    num_clusters = 3\n",
    "\n",
    "    # Apply KMeans clustering algorithm\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    clusters = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "    # Calculate the distances from each point to its cluster centroid\n",
    "    distances = kmeans.transform(data_scaled)\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "\n",
    "    # Determine the threshold for the top 5% farthest points\n",
    "    threshold = np.percentile(min_distances, 100-100*per_anomalies)\n",
    "\n",
    "    # Label points as anomalies if their distance is greater than the threshold\n",
    "    anomalies = min_distances > threshold\n",
    "    predicted_labels = anomalies.astype(int)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    # Calculate precision and recall\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    print('F1 score till now:',f1)\n",
    "    print('Precision score till now:',precision)\n",
    "    print('Recall score till now:',recall)\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
    "\n",
    "    # print confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"True Negatives:\", tn)\n",
    "    print(\"False Positives:\", fp)\n",
    "    print(\"False Negatives:\", fn)\n",
    "    print(\"True Positives:\", tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo2(blk_num,input_data,true_labels, per_anomalies):\n",
    "    print('Inside kNN algo with input length: ', len(blk_num))\n",
    "    # Convert the input data to a NumPy array\n",
    "    data = np.array(input_data)\n",
    "\n",
    "    # Scale the data to normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # Set the number of nearest neighbors to consider\n",
    "    k = 5\n",
    "\n",
    "    # Calculate the distances to the K nearest neighbors for each data point\n",
    "    nearest_neighbors = NearestNeighbors(n_neighbors=k+1)  # +1 to exclude the point itself\n",
    "    nearest_neighbors.fit(data_scaled)\n",
    "    distances, _ = nearest_neighbors.kneighbors(data_scaled)\n",
    "\n",
    "    # Get the average distance to the K nearest neighbors (excluding the point itself)\n",
    "    avg_distances = np.mean(distances[:, 1:], axis=1)\n",
    "\n",
    "    # Determine the threshold for the top 5% farthest points\n",
    "    threshold = np.percentile(avg_distances, 100-100*per_anomalies)\n",
    "\n",
    "    # Label points as anomalies if their average distance is greater than the threshold\n",
    "    anomalies = avg_distances > threshold\n",
    "\n",
    "    # Convert the boolean anomaly flags to binary values (1 for anomaly, 0 for normal)\n",
    "    predicted_labels = anomalies.astype(int)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average = 'macro')\n",
    "    precision = precision_score(true_labels, predicted_labels,average = 'macro')\n",
    "    recall = recall_score(true_labels, predicted_labels,average = 'macro')\n",
    "    print('F1 score till now:',f1)\n",
    "    print('Precision score till now:',precision)\n",
    "    print('Recall score till now:',recall)\n",
    "    # calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
    "\n",
    "    # print confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"True Negatives:\", tn)\n",
    "    print(\"False Positives:\", fp)\n",
    "    print(\"False Negatives:\", fn)\n",
    "    print(\"True Positives:\", tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo3(blk_num,input_data,true_labels, per_anomalies):\n",
    "    print('Inside DBSCAN clustering with input length: ', len(blk_num))\n",
    "  \n",
    "    # Convert the input data to a NumPy array\n",
    "    data = np.array(input_data)\n",
    "\n",
    "    # Scale the data to normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # Apply DBSCAN clustering algorithm\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=2)  # Tune these parameters based on your data\n",
    "    clusters = dbscan.fit_predict(data_scaled)\n",
    "\n",
    "    # Calculate the distances to the nearest neighbor for each data point\n",
    "    nearest_neighbors = NearestNeighbors(n_neighbors=2)\n",
    "    nearest_neighbors.fit(data_scaled)\n",
    "    distances, _ = nearest_neighbors.kneighbors(data_scaled)\n",
    "\n",
    "    # Get the distances to the nearest neighbor (excluding itself)\n",
    "    nearest_distances = distances[:, 1]\n",
    "\n",
    "    # Determine the threshold for the top 5% farthest points\n",
    "    threshold = np.percentile(nearest_distances, 100-(100*per_anomalies))\n",
    "\n",
    "    # Label points as anomalies if their distance is greater than the threshold\n",
    "    anomalies = nearest_distances > threshold\n",
    "\n",
    "    predicted_labels = anomalies.astype(int)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average = 'macro')\n",
    "    precision = precision_score(true_labels, predicted_labels,average = 'macro')\n",
    "    recall = recall_score(true_labels, predicted_labels,average = 'macro')\n",
    "    print('F1 score till now:',f1)\n",
    "    print('Precision score till now:',precision)\n",
    "    print('Recall score till now:',recall)\n",
    "    # calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
    "\n",
    "    # print confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"True Negatives:\", tn)\n",
    "    print(\"False Positives:\", fp)\n",
    "    print(\"False Negatives:\", fn)\n",
    "    print(\"True Positives:\", tp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
